{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c55fac",
   "metadata": {},
   "source": [
    "## Importando Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc69718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "import re\n",
    "import PyPDF2\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "import stanza\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb31dd73",
   "metadata": {},
   "source": [
    "## Criando a TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce6cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TextRank4Keyword Reference: https://gist.github.com/BrambleXu/3d47bbdbd1ee4e6fc695b0ddb88cbf99\n",
    "nlp = spacy.load('pt_core_news_lg')\n",
    "nlp2 = stanza.Pipeline(lang='pt', processors='tokenize,mwt,pos,lemma') \n",
    "\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        all_keys = []\n",
    "        \n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            all_keys.append(key)\n",
    "            if i > number:\n",
    "                break\n",
    "               \n",
    "        return all_keys\n",
    "    \n",
    "    def lemmatize(self, text):\n",
    "        doc = nlp2(text)\n",
    "        lemmatized_words = []\n",
    "        stopwords_list = nltk.corpus.stopwords.words('portuguese')\n",
    "        ignored_postag = ['DET', 'NUM', 'ADV', 'SCONJ', 'CCONJ', 'DET', 'AUX', 'ADP', 'PRON','VERB']\n",
    "        \n",
    "        for word in doc.iter_words():\n",
    "            if word.upos not in ignored_postag and word.lemma not in stopwords_list:\n",
    "                lemmatized_words.append(word.lemma.lower())\n",
    "        \n",
    "        return \" \".join(lemmatized_words)\n",
    "\n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        #Detect compound noun\n",
    "        #text = self.detect_compound(text) Needs a better compound noun detection (The last version had a low detection rate in portuguese)\n",
    "        \n",
    "        #Lemmatization\n",
    "        text = self.lemmatize(text)\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            if word != '_':\n",
    "                node_weight[word] = pr[index]\n",
    "                \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87b8ea9",
   "metadata": {},
   "source": [
    "## Gerando keywords dos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set year and text_type or create a new file structure\n",
    "year = '2021'\n",
    "text_type = 'plano'\n",
    "files = os.listdir(f'{year}/{text_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf778cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(full_raw_text):\n",
    "    full_raw_text = re.sub(r\"\\(.*?\\)\", '', full_raw_text)\n",
    "    full_raw_text = re.sub(r\"([A-Z]+)\\s?\\,\\s?([A-Za-z\\s]+)[\\.\\;]\", '', full_raw_text) \n",
    "    full_raw_text = re.sub(r\"([Dd]ispon[iÃ­]vel\\s[Ee]m|[Aa]vailable\\s[Ff]rom)\\s?:\", '', full_raw_text)\n",
    "    full_raw_text = re.sub(r\"\\<?((https?)?:\\/\\/|www)\\S+\", '', full_raw_text)\n",
    "    full_raw_text = re.sub(r\"[0-9]\", '', full_raw_text)\n",
    "\n",
    "    full_raw_text = full_raw_text.replace('\\n','')\n",
    "\n",
    "\n",
    "    punctuations = ['!','@','#','$','%','&','*','_','+','=','<','>',',','.',';',':','?','/','|','(',')']\n",
    "\n",
    "    for k in range(len(punctuations)):\n",
    "        full_raw_text  = full_raw_text.replace(punctuations[k], '')\n",
    "\n",
    "    full_raw_text = full_raw_text.lower() \n",
    "    \n",
    "    return full_raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d3936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"keywords_all_year.json\") #YOU NEED TO HAVE A JSON FILE TO USE (PREFERABLY EMPTY)\n",
    "\n",
    "all_text_year = list(json.load(f))\n",
    "print(type(all_text_year))\n",
    "\n",
    "\n",
    "for i in files:\n",
    "    text_dict = {}\n",
    "    \n",
    "    print(i)\n",
    "    file = open(f'{year}/{text_type}/{i}', 'rb')\n",
    "    pdf = PyPDF2.PdfFileReader(file)\n",
    "    pages_text = []\n",
    "\n",
    "\n",
    "    for j in range(pdf.getNumPages()):\n",
    "        page = pdf.getPage(j)\n",
    "        pages_text.append(page.extractText())\n",
    "\n",
    "    full_raw_text = \"\".join(pages_text)\n",
    "    full_raw_text = text_cleaning(full_raw_text)\n",
    "        \n",
    "    stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "    tr4w = TextRank4Keyword()\n",
    "    tr4w.analyze(full_raw_text, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False, stopwords=stopwords)\n",
    "    \n",
    "    text_dict['year'] = year\n",
    "    text_dict['text_type'] = text_type\n",
    "    text_dict['name'] = i\n",
    "    text_dict['keywords'] = tr4w.get_keywords(150)\n",
    "    \n",
    "    all_text_year.append(text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190a0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing json \n",
    "json_object = json.dumps(all_text_year, indent = 4)\n",
    "  \n",
    "# Writing to keywords_all_year.json\n",
    "with open(\"keywords_all_year.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eeaf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_text_year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
